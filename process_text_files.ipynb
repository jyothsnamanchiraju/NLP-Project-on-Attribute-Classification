{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"process_text_files.ipynb","provenance":[],"authorship_tag":"ABX9TyMkgiGUgBtLxDra8psJfuG2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Preparing the text in 'Description' column of \"bookSynopsis_ageRange_clean.csv\" file.\n","The file is clean with respect to 'Reading_age', 'lower_age', 'upper_age' columns. While the 'Description' column should be cleaned and the data must be prepared for feature extraction.\n","\n","Ref. https://www.analyticsvidhya.com/blog/2020/11/text-cleaning-nltk-library/"],"metadata":{"id":"YuvRdUNygLL4"}},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RvtyaDz0g8r9","executionInfo":{"status":"ok","timestamp":1648269960376,"user_tz":-330,"elapsed":18268,"user":{"displayName":"Jyothsna Sarvadevabhatla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03749561797508157750"}},"outputId":"1d9ad0f3-e59c-4567-b049-48a5491c5d66"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["import pandas as pd \n","\n","book_synopsis = pd.read_csv(\"/gdrive/My Drive/MSC_Thesis_Project/Data/bookSynopsis_ageRange_clean.csv\")\n","\n","#removing extra spaces in the book_synopsis dataframe\n","import re \n","desc=\"\"\n","\n","for i in book_synopsis.index: \n","     try: \n","          desc = re.sub(\"\\s+\",\" \",book_synopsis['Desc'][i])\n","          book_synopsis['Desc'][i] = desc\n","     except:     \n","          book_synopsis['Desc'][i]= book_synopsis['Title'][i]\n","          desc = re.sub(\"\\s+\",\" \",book_synopsis['Desc'][i])\n","          book_synopsis['Desc'][i] = desc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MPbovYu3jqJN","executionInfo":{"status":"ok","timestamp":1648269962347,"user_tz":-330,"elapsed":1977,"user":{"displayName":"Jyothsna Sarvadevabhatla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03749561797508157750"}},"outputId":"2b24e123-763a-4900-9b00-fa8f470dabcb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if sys.path[0] == '':\n"]}]},{"cell_type":"code","source":["book_synopsis['Desc']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mp_PH00J0i55","executionInfo":{"status":"ok","timestamp":1648269962348,"user_tz":-330,"elapsed":8,"user":{"displayName":"Jyothsna Sarvadevabhatla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03749561797508157750"}},"outputId":"db7a0554-0fb2-4301-f8e4-a5e52d88515d"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       Little Ross was sad. He didnt have a nice new ...\n","1       Abe was a real Service Dog who dedicated his l...\n","2       When the last bubble popped, he was gone but a...\n","3       Dr. Zobs evil plans had failed once again, tha...\n","4       Those little beauties have inspired so many st...\n","                              ...                        \n","1576     Discover the True You! Who is the real you? Y...\n","1577     Celebrate your little cuddle bug with this co...\n","1578     Your baby's first word will be . . .\"Dada!\"Ri...\n","1579     By You, about You, for You! Your Diary is all...\n","1580     Follow the adventures of Zoey and her cat Sas...\n","Name: Desc, Length: 1581, dtype: object"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Removing Punctuations \n","desc=\"\"\n","for i in book_synopsis.index: \n","    desc = re.sub(\"[^-9A-Za-z ]\",\"\",book_synopsis['Desc'][i])\n","    book_synopsis['Desc'][i] = desc\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tEczcHx6rShC","executionInfo":{"status":"ok","timestamp":1648269963104,"user_tz":-330,"elapsed":763,"user":{"displayName":"Jyothsna Sarvadevabhatla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03749561797508157750"}},"outputId":"486af7f5-da22-4575-e62b-ca1546636685"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"\n"]}]},{"cell_type":"code","source":["#Turning text into lower case \n","\n","import string \n","desc=\"\"\n","txt = \"\"\n","\n","for i in book_synopsis.index: \n","    txt  = book_synopsis['Desc'][i]\n","    desc = \"\".join([i.lower() for i in txt if i not in string.punctuation])\n","    book_synopsis['Desc'][i] = desc\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZwnmL2T4x1T","executionInfo":{"status":"ok","timestamp":1648269963707,"user_tz":-330,"elapsed":608,"user":{"displayName":"Jyothsna Sarvadevabhatla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03749561797508157750"}},"outputId":"82f3e02f-b9d0-421e-8106-188e13bae994"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # Remove the CWD from sys.path while we load stuff.\n"]}]},{"cell_type":"code","source":["#Removing Stopwords\n","import nltk \n","import json\n","\n","\n","nltk.download('stopwords') \n","nltk.download('wordnet')\n","nltk.download('punkt')\n","stopwords = nltk.corpus.stopwords.words('english')\n","\n","desc=\"\"\n","txt =[]\n","desc_words = []\n","\n","#lemmetizer object \n","wn = nltk.WordNetLemmatizer()\n","lem_words = []\n","\n","\n","#Creating Json file \n","words = [] \n","lage = 0\n","uage = 0\n","json_records_list= []      #{words:[],lage:, uage: }\n","curr_record = {}\n","\n","for i in book_synopsis.index: \n","    curr_record = {}\n","    desc = book_synopsis['Desc'][i]\n","    txt = nltk.tokenize.word_tokenize(desc)                     # tokenize into words\n","    desc_words = [i for i in txt if i not in stopwords]         # remove stop words\n","   \n","    curr_record['desc'] = desc_words\n","    curr_record['lower_age'] = book_synopsis['lower_age'][i]\n","    curr_record['upper_age'] = book_synopsis['upper_age'][i]\n","    json_records_list.append(curr_record)\n","  \n","!pip install numpyencoder\n","from numpyencoder import NumpyEncoder\n","\n","json_records = {}\n","json_records['dataset'] = json_records_list\n","\n","#with open('/gdrive/My Drive/MSC_Thesis_Project/Data/cleanSynopsis_ageRange.json', 'w') as fp:\n","#    json.dump(json_records, fp, cls=NumpyEncoder)\n","\n","#below code is to write records with non-lemmatized words from the Book Synopsis\n","with open('/gdrive/My Drive/MSC_Thesis_Project/Data/cleanSynopsis_ageRange_nonlemm.json', 'w') as fp:\n","    json.dump(json_records, fp, cls=NumpyEncoder)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GjtRa3NU7FYM","executionInfo":{"status":"ok","timestamp":1648269972071,"user_tz":-330,"elapsed":8369,"user":{"displayName":"Jyothsna Sarvadevabhatla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03749561797508157750"}},"outputId":"07d69907-69d9-464d-abb0-2242e5623a2f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Collecting numpyencoder\n","  Downloading numpyencoder-0.3.0-py3-none-any.whl (3.0 kB)\n","Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from numpyencoder) (1.21.5)\n","Installing collected packages: numpyencoder\n","Successfully installed numpyencoder-0.3.0\n"]}]},{"cell_type":"code","source":["# Read Json \n","\n","# Open the orders.json file\n","with open(\"/gdrive/My Drive/MSC_Thesis_Project/Data/cleanSynopsis_ageRange.json\") as file:\n","    # Load its content and make a new dictionary\n","    data = json.load(file)\n","\n","data['dataset'][12]['desc']\n","    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUWkd95PNGiG","executionInfo":{"status":"ok","timestamp":1648269972668,"user_tz":-330,"elapsed":605,"user":{"displayName":"Jyothsna Sarvadevabhatla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03749561797508157750"}},"outputId":"341c1850-1c25-4ea6-88da-8d65d68d5c03"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['jamal',\n"," 'made',\n"," 'case',\n"," 'next',\n"," 'open',\n"," 'house',\n"," 'mattered',\n"," 'much',\n"," 'download',\n"," 'free',\n"," 'childrens',\n"," 'book',\n"," 'read',\n"," 'chose',\n"," 'stand',\n"," 'ground',\n"," 'took',\n"," 'courage',\n"," 'validate',\n"," 'action',\n"," 'little',\n"," 'bit',\n"," 'honesty',\n"," 'little',\n"," 'bit',\n"," 'courage',\n"," 'gone',\n"," 'long',\n"," 'way',\n"," 'boosting',\n"," 'jamals',\n"," 'belief',\n"," 'read',\n"," 'free',\n"," 'childrens',\n"," 'real',\n"," 'aloud',\n"," 'truly',\n"," 'inspired']"]},"metadata":{},"execution_count":7}]}]}