{"cells":[{"cell_type":"markdown","metadata":{"id":"1WsRRXTbiA4V"},"source":["Extracting Statistical features like - \n","1. TF-IDF\n","2. Doc2Vec \n","3. Word2Vec \n","4. Bert\n","5. Handcrafted(ref to extract_handcrafted_feature.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23649,"status":"ok","timestamp":1649603992211,"user":{"displayName":"Jyothsna Sarvadevabhatla","userId":"03749561797508157750"},"user_tz":-330},"id":"rsZN_r0NjIOB","outputId":"aec78311-1651-4ff4-fe58-6397a26f0b90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive \n","drive.mount('/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"PLIplu7SilF8"},"source":["# TF-IDF (Ref. for Text_hero https://stackoverflow.com/questions/37593293/how-to-get-tfidf-with-pandas-dataframe)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":1026,"status":"error","timestamp":1649603996716,"user":{"displayName":"Jyothsna Sarvadevabhatla","userId":"03749561797508157750"},"user_tz":-330},"id":"ZiprmNVziXol","outputId":"9aa5cbb4-682e-499c-b1aa-96c501d042c2"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-87fb165407bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Open the cleanSynopsis_ageRange.json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdrive/My Drive/MSC_Thesis_Project/Data/cleanSynopsis_ageRange.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Load its content and make a new dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlemm_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gdrive/My Drive/MSC_Thesis_Project/Data/cleanSynopsis_ageRange.json'"]}],"source":["# Load the lemmatized features - \n","import pandas as pd \n","import json\n","## json structure of - cleanSynopsis_ageRange.json = \n","##{'dataset': [{  'desc': [\"text1\", \"text2\"]\n","##              'lower_age': int\n","##              'upper_age': int\n","##}]\n","##    \n","##}\n","\n","\n","# Read Json \n","\n","# Open the cleanSynopsis_ageRange.json file\n","with open(\"/gdrive/My Drive/MSC_Thesis_Project/Data/cleanSynopsis_ageRange.json\") as file:\n","    # Load its content and make a new dictionary\n","    lemm_data = json.load(file)\n","\n","#lemm_data['dataset'][12]['desc']\n","\n","# on lemmatized\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer \n","\n","documents_df=pd.DataFrame(lemm_data['dataset'],columns=['desc','desc_sentences'])\n","\n","#concatenating all rows of lemmatized data into a series of words - \n","allSentences= []\n","sentence = \"\"\n","for i in documents_df.index:\n","         sentence = \"\"\n","         for a in  documents_df['desc'][i]:\n","             sentence =  sentence + a +\" \" \n","         documents_df['desc_sentences'][i] = sentence\n","         #allSentences.append(sentence) \n","\n","#documents_df\n","!pip install texthero\n","\n","import texthero as hero\n","documents_df['tfidf'] = hero.tfidf(documents_df['desc_sentences'])\n","\n","documents_df.head()\n","#tfidfvectoriser=TfidfVectorizer()\n","#tfidfvectoriser.fit(allSentences)\n","#tfidf_vectors=tfidfvectoriser.transform(allSentences)\n","#matrix = tfidfvectoriser.fit_transform(allSentences)\n","\n","\n","#matrix\n","#print(\"Feature Names \",len(tfidfvectoriser.get_feature_names_out()))\n","#tfidf_vectors "]},{"cell_type":"markdown","metadata":{"id":"i9blW2uUocIj"},"source":["Conclusion from TF-IDF - \n","1. the outcome of tfidf column in the above code gave too many 0.0s \n","2. This is because none of the words from one description match with other. \n","3. Hence, we don't want to use tfidf :"]},{"cell_type":"markdown","metadata":{"id":"iHJKERJgrERF"},"source":["Word2vec - "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tg97_PV6scAx","executionInfo":{"status":"ok","timestamp":1649604038249,"user_tz":-330,"elapsed":966,"user":{"displayName":"Jyothsna Sarvadevabhatla","userId":"03749561797508157750"}},"outputId":"fdd0703b-366d-4a2d-dbf5-358128576e2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-10 15:20:31--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.80.211\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.80.211|:443... connected.\n","HTTP request sent, awaiting response... 404 Not Found\n","2022-04-10 15:20:31 ERROR 404: Not Found.\n","\n"]}],"source":["#Loading non-lemmatized data from 'cleanSynopsis_ageRange_nonlemm.json'\n","# Opening the file \n","with open(\"/gdrive/My Drive/MSC_Thesis_Project/Data/cleanSynopsis_ageRange_nonlemm.json\") as file:\n","    # Load its content and make a new dictionary\n","    data = json.load(file)\n","\n","#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n","\n","#installing word2vec dictionary\n","\n","!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","     # https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHvECY7lhksU","executionInfo":{"status":"ok","timestamp":1649604163609,"user_tz":-330,"elapsed":4554,"user":{"displayName":"Jyothsna Sarvadevabhatla","userId":"03749561797508157750"}},"outputId":"56ba8f0a-bb3f-4153-a17f-4769e5d3edc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"]}],"source":["##library for text- processing \n","!pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_FuYd2LhzgD"},"outputs":[],"source":["# loading pre-trained embeddings, each word is represented as a 300 dimensional vector\n","import gensim\n","model = gensim.models.KeyedVectors.load_word2vec_format('/gdrive/My Drive/MSC_Thesis_Project/Data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_J2T2AwiB-C"},"outputs":[],"source":["def filter_docs(corpus, texts, condition_on_doc):\n","    # Filter corpus, texts and labels given the function condition_on_doc which takes\n","    #a doc.\n","    #The document doc is kept if condition_on_doc(doc) is true.\n","    \n","    number_of_docs = len(corpus)\n","\n","    if texts is not None:\n","        texts = [text for (text, doc) in zip(texts, corpus)\n","                 if condition_on_doc(doc)]\n","\n","    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n","\n","    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n","\n","    return (corpus, texts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leMUm4eqiRdL"},"outputs":[],"source":["import numpy as np \n","\n","def document_vector(word2vec_model, doc):\n","    # remove out-of-vocabulary words\n","    doc = [word for word in doc if word in word2vec_model.vocab]\n","    return np.mean(word2vec_model[doc], axis=0)\n","   # return np.sum(word2vec_model[doc], axis=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrQ6M-fqibC7"},"outputs":[],"source":["def has_vector_representation(word2vec_model, doc):\n","    \"\"\"check if at least one word of the document is in the\n","    word2vec dictionary\"\"\"\n","    return not all(word not in word2vec_model.vocab for word in doc)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yI1FboGijPZu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649604380538,"user_tz":-330,"elapsed":947,"user":{"displayName":"Jyothsna Sarvadevabhatla","userId":"03749561797508157750"}},"outputId":"67fec2df-a814-461e-9772-481c3d955004"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1581, 300)"]},"metadata":{},"execution_count":10}],"source":["docs_df=pd.DataFrame(data['dataset'],columns=['desc'])\n","\n","x =[]\n","for i in docs_df.index: #look up each doc in model\n","    x.append(document_vector(model, docs_df['desc'][i]))\n","    \n","X= np.array(x)\n","\n","np.shape(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sB2PxK94nK8k"},"outputs":[],"source":["# Saving the word2vec features - \n","np.save('/gdrive/My Drive/MSC_Thesis_Project/Data/word2vec_mean_features', X)  #will be saved as .npy file"]}],"metadata":{"colab":{"name":"extract_statistical_features.ipynb","provenance":[],"authorship_tag":"ABX9TyMT28GSDvi6KCP1pKjPMrDb"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}