{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extract_handcrafted_features.ipynb","provenance":[],"authorship_tag":"ABX9TyP21BY0cI2k9MRYLUxdef8r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","Handcrafted Feature extraction - \n","1. \n","\n"],"metadata":{"id":"z7wnTtz8Gch0"}},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/gdrive')\n","\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA   #Sentiment Intensity Analyser\n","import nltk\n","nltk.download('vader_lexicon')  \n","\n","\n","!pip install textatistic\n","from textatistic import Textatistic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5YGUNGbGiEy","executionInfo":{"status":"ok","timestamp":1649348547309,"user_tz":-330,"elapsed":29973,"user":{"displayName":"Jyothsna Sarvadevabhatla","userId":"03749561797508157750"}},"outputId":"fbd60de5-d277-4b6a-cadb-d0664fb4d5f2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"]},{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","Collecting textatistic\n","  Downloading textatistic-0.0.1.tar.gz (29 kB)\n","Collecting pyhyphen>=2.0.5\n","  Downloading PyHyphen-4.0.3.tar.gz (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.36.0 in /usr/local/lib/python3.7/dist-packages (from pyhyphen>=2.0.5->textatistic) (0.37.1)\n","Requirement already satisfied: setuptools>=52.0 in /usr/local/lib/python3.7/dist-packages (from pyhyphen>=2.0.5->textatistic) (57.4.0)\n","Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pyhyphen>=2.0.5->textatistic) (1.4.4)\n","Collecting requests>=2.25\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (1.24.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2.0.12)\n","Building wheels for collected packages: textatistic, pyhyphen\n","  Building wheel for textatistic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for textatistic: filename=textatistic-0.0.1-py3-none-any.whl size=29068 sha256=833802cbd33fb538a1b42063012ba1394abb5fef02b3ebb9feac009dc81ecca1\n","  Stored in directory: /root/.cache/pip/wheels/58/4a/1a/5ed2a089cbd2f98693b07221c4ab499c8c446e15b6123ba4a4\n","  Building wheel for pyhyphen (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyhyphen: filename=PyHyphen-4.0.3-cp37-abi3-linux_x86_64.whl size=60312 sha256=b34b9069c5a84aa1ac04b872ce2ed5c04dc50ce3df999081775365d372ef1f73\n","  Stored in directory: /root/.cache/pip/wheels/4e/21/3e/e883a6e9969fdd074763213ddaeee0e781c359bbfda3fa435f\n","Successfully built textatistic pyhyphen\n","Installing collected packages: requests, pyhyphen, textatistic\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed pyhyphen-4.0.3 requests-2.27.1 textatistic-0.0.1\n"]}]},{"cell_type":"code","source":["#counting length of sentences \n","\n","def sentLen(s):\n","  return len(s.split())\n","\n"],"metadata":{"id":"s5-A_DI-eSBR","executionInfo":{"status":"ok","timestamp":1649348555458,"user_tz":-330,"elapsed":349,"user":{"displayName":"Jyothsna Sarvadevabhatla","userId":"03749561797508157750"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import pandas as pd \n","import re\n","\n","book_synopsis = pd.read_csv(\"/gdrive/My Drive/MSC_Thesis_Project/Data/bookSynopsis_ageRange_clean.csv\") \n","\n","#the row 1190 has NAN for desc\n","#therefore giving the desc same as title \n","#book_synopsis['Desc'][1190] = book_synopsis['Title'][1190]\n","\n","t= \"\"\n","d=\"\"\n","words = []\n","wordCount=0\n","avgWordLen = 0\n","sentences=[]\n","sentenceCount=0\n","avgSentLen=0\n","\n","sentiments=[]\n","pos_sentiment=0.0\n","neu_sentiment=0.0\n","neg_sentiment=0.0\n","\n","tr_delechall = 0.0\n","tr_flesch_score = 0.0\n","tr_fleschkincaid_score = 0.0\n","tr_gunningfog_score= 0.0\n","tr_smog_score= 0.0\n","\n","desc_readability_scores= []\n","\n","col_names=['Title','wordCount','avgWordLen','sentenceCount','avgSentLen','pos_sentiment','neu_sentiment','neg_sentiment',\n","           'tr_delechall','tr_flesch_score','tr_fleschkincaid_score','tr_gunningfog_score','tr_smog_score']\n","handcrafted_features = pd.DataFrame(columns = col_names)\n","\n","for i in book_synopsis.index: \n","    t = book_synopsis['Title'][i]\n","    d = book_synopsis['Desc'][i]\n","    words = d.split()\n","    #feature - number of words \n","    wordCount= len(words)\n","    #feature - average length of words \n","    avgWordLen = sum(len(word) for word in words) / len(words) \n","    #feature - sentence count \n","    d= re.sub('\\.+', '.', d)      #replacing multiple '...' with '.'\n","    d= d.replace('\\n','')         #replacing '\\n' with ''\n","    if not (d.endswith('.')):     #always ensure that the 'desc' ends with '.'\n","       d= d+'.' \n","    sentences= d.split(\".\")\n","    sentenceCount= len(sentences)\n","    #feature - average sentence length\n","    avgSentLen = sum(sentLen(s) for s in sentences) / sentenceCount   #formula avg = sum(len(sentence))/ total no.of sentences\n","\n","    #Sentiment Analysis (positive, negative & neutral)\n","    sina = SIA()    #Sentiment Intensity Analyser\n","    sentiments =    sina.polarity_scores(' '.join(re.findall(r'\\w+',d.lower())))\n","    \n","    pos_sentiment  = sentiments['pos']+1*(10**-6) \n","    neu_sentiment  = sentiments['neu']+1*(10**-6)\n","    neg_sentiment  = sentiments['neg']+1*(10**-6)\n","\n","   #Text readability scores -  \n","    \n","    desc_readability_scores = Textatistic(d).scores \n","      \n","\n","    tr_delechall           = desc_readability_scores['dalechall_score'] \t\n","    tr_flesch_score        = desc_readability_scores['flesch_score']\n","    tr_fleschkincaid_score = desc_readability_scores['fleschkincaid_score']\n","    tr_gunningfog_score\t   = desc_readability_scores['gunningfog_score']\n","    tr_smog_score          = desc_readability_scores['smog_score']\n","\n","\n","    handcrafted_features = handcrafted_features.append({\n","                                 'Title'    : t,\n","                                 'wordCount': wordCount,\n","                                 'avgWordLen': avgWordLen,\n","                                 'sentenceCount': sentenceCount,\n","                                 'avgSentLen'   : avgSentLen,\n","                                 'pos_sentiment': pos_sentiment,\n","                                 'neu_sentiment': neu_sentiment,\n","                                 'neg_sentiment': neg_sentiment,\n","                                  'tr_delechall': tr_delechall,\n","                               'tr_flesch_score': tr_flesch_score,\n","                        'tr_fleschkincaid_score': tr_fleschkincaid_score,\n","                           'tr_gunningfog_score': tr_gunningfog_score,\n","                                 'tr_smog_score': tr_smog_score\n","                              }, ignore_index= True)\n","##\n","##Writing Handcrafted features to csv file handcrafted_features.csv\n","##\n","handcrafted_features.to_csv('/gdrive/My Drive/MSC_Thesis_Project/Data/handcrafted_features.csv')\n"],"metadata":{"id":"8HfXNSOXGvMz","executionInfo":{"status":"ok","timestamp":1649348610027,"user_tz":-330,"elapsed":52394,"user":{"displayName":"Jyothsna Sarvadevabhatla","userId":"03749561797508157750"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#checking whether all rows match in both dataframes \n","#book_synopsis & handcrafted_features - \n","t= \"\" \n","c = 0\n","for i in book_synopsis.index:\n","    t = handcrafted_features.iloc[i]['Title'] \n","    if not(t==book_synopsis['Title'][i]):\n","        print(str(i), book_synopsis['Title'][i])\n","    else:\n","        c = c+1\n","\n","\n","\n","print(\"c = \"+str(c))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTDFQStD0lAV","executionInfo":{"status":"ok","timestamp":1649348617545,"user_tz":-330,"elapsed":366,"user":{"displayName":"Jyothsna Sarvadevabhatla","userId":"03749561797508157750"}},"outputId":"80485b0f-0c4c-4f52-b506-d27db4e8f5f7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["c = 1581\n"]}]},{"cell_type":"markdown","source":["# Ref. https://goodboychan.github.io/python/datacamp/natural_language_processing/2020/07/17/01-Basic-features-and-readability-scores.html#Readability-tests\n","# Readability Tests\n"],"metadata":{"id":"7_qqqqVswdgp"}}]}